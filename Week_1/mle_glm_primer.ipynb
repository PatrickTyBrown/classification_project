{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimators: The Big Picture\n",
    "\n",
    "The big question here is where do cost functions come from and can/how do we motivate them from rigorous probabilistic assumptions?\n",
    "\n",
    "To answer these questions, we take a step back into theory land and express the selection of **parametric models** (e.g. ones that can be described as a collection of coefficients like linear or logistic regression) in terms of probability. \n",
    "\n",
    "If we have observed a collection of **features and targets $X, y$** and want to choose a parametric model $\\theta$ that best describes the observed data, we can abstractly express this goal as **trying to maximize**:\n",
    "\n",
    "$$P(\\theta|X, y) $$ \n",
    "\n",
    "i.e. of all possible models, choosing the model that is the most probable  given the data that's available to us.\n",
    "\n",
    "We immediately want to apply **Bayes theorem** to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\theta|X, y) = \\frac{P(X,y | \\theta)P(\\theta)}{P(X,y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then take it a step further with conditional probability calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(X,y | \\theta) = P(y | X, \\theta)P(X | \\theta) $$\n",
    "\n",
    "$$ = C P(y | X, \\theta)$$,\n",
    "\n",
    "where $C$ is some constant that we will never practically be able to calculate directly, and will ignore for the purposes of optimization.\n",
    "\n",
    "We define $P(y | X, \\theta)$ as the **likelihood of $\\theta$** aka $L(\\theta | X)$, and from the math above now know that finding theta to maximize the probability of theta given the data we've observed is equivalent to **maximizing this likelihood quantity**, i.e. finding theta to maximize the probability of the data we've observed given that theta (note the order flipping in this statement).\n",
    "\n",
    "This reformulation is a great benefit to us and allows us to actually build models, because once we **choose a model by imposing probabilistic and functional form assumptions**, we can then explicitly calculate $P(y | X, \\theta)$ for any candidate value of $\\theta$. In particular, we can apply **numerical optimization techniques** (gradient descent or equivalent), in order to find the value of $\\theta$ that maximizes likelihood.\n",
    "\n",
    "That takes us to--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe for Maximum Likelihood Estimators\n",
    "\n",
    "1. Choose proper conditional distribution for $p(y | x)$ (i.e. one feature/target pair)\n",
    "2. Express $E(y | x)$ (expected value of distribution) as a function of the features of x, involving the parameters $\\theta$\n",
    "3. **\"Train\" the model** by running an optimization process that chooses $\\theta$ to maximize $L(\\theta)$ as dervied from steps 1 and 2\n",
    "\n",
    "Below is a visual showcasing steps 1-2 for linear regression: we see 3 different conditional distributions for $y$ corresponding to different values of $x$, each centered around a point on the best fit line, which is the functional form we chose ($E(y | x)$ is a linear combination of the features).\n",
    "\n",
    "![linear_conditionals](linear_conditionals.png)\n",
    "\n",
    "### Linear Regression Example\n",
    "\n",
    "1. $p(y | x) \\sim N(\\mu,\\sigma)$ (Normal distribution)\n",
    "2. $E(y | x) = \\mu = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p$ (the usual linear combination functional form)\n",
    "3. Run gradient descent to choose optimal $\\theta = (\\beta_0, \\beta_1, ... \\beta_p)$ to maximize $L(\\theta)$, which ends up being equivalent to **minimizing SSE loss**! For a derivation of this, check out the linear regression MLE notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where do \"Generalized Linear Models\" fit in?\n",
    "\n",
    "**GLM** just refers to a specific class of **MLE** models, where the link between the expected value of the target's conditional distribution and the features involves a **linear combination**. For example, **logistic regression** is a **GLM** because $E(y | x) = p = \\sigma(\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p)$ i.e. a linear combination passed through the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
