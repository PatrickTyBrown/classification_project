{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command line install, uncomment line below\n",
    "# !conda install -y -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "# see https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "%pylab inline\n",
    "\n",
    "# sets backend to render higher res images\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "#######################\n",
    "#       imports       #\n",
    "#######################\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptual Overview & Hyperparameter Tuning with xgboost\n",
    "\n",
    "**Objectives**:\n",
    "1. Understand the basic mathematical framework behind gradient boosted trees and the model's key hyperparameters\n",
    "2. Understand the typical use cases for gradient boosting and some of the pros and cons\n",
    "3. Build familiarity with the widely used xgboost package and practice selecting/tuning hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Framework\n",
    "\n",
    "**First Pass (Regression)**:\n",
    "\n",
    "Let's start by briefly returning to random forests. With the RF algorithm, we start with a very low bias but high variance **strong learner**, and reduce its variance by averaging across many relatively uncorrelated copies. Recall that we get these copies by injecting randomness into each base tree in the forest ensemble (taking bootstrapped samples of the training data, subsampling the feature columns for each tree split). The end result is a model that overfits less than a single decision tree due to variance reduction. See the diagram from [KDnuggets](https://www.kdnuggets.com/2017/10/random-forests-explained.html) below.\n",
    "\n",
    "![](images/RFDiagram.jpg)\n",
    "\n",
    "Boosting models are a flipped version of this paradigm. Instead of a strong learner, we start with a high bias but low variance **weak learner** and reduce its bias by training models that iteratively correct for the mistakes made by previous models. Building a sort of tower of weak models in this fashion makes for a strong ensemble that doesn't underfit, and the low variance of each base model helps insulate the ensemble from overfitting.\n",
    "\n",
    "The most powerful variety of boosting in modern ML is **gradient boosted trees**, which we'll focus on. Our base learner is a simple decision tree with relatively small depth / few decision splits. We will build a final model by adding the results of many such trees together. The most intuitive problem setting to start with here is regression, with the usual sum of the squared errors (SSE) as the loss function we aim to minimize for data $X, y$. In each step of model building, we fit a base learner tree to the residuals of the current model's training predictions, adding it to the ensemble and gradually decreasing our training error. \n",
    "\n",
    "In (mostly) english, here's what we do:\n",
    "\n",
    "* We'll train $k$ (hyperparameter) base learner decision trees with max depth $d$ (hyperparameter) step by step, adding each to our overall model.\n",
    "* Each base tree is fit using our data $X$, but the target values are the residuals $y - model_{cur}$, where $model_{cur}$ is our overall model at that time step (sum of previous trees and initialization)  \n",
    "* We initialize our model with the best constant prediction, $mean(y)$.   \n",
    "\n",
    "As an algorithm:\n",
    "\n",
    "1. Set $T_0 = mean(y)$\n",
    "2. For $m = 1,...,k$:\n",
    "    1. Set $r_{m-1} = y - (T_0 + \\sum\\limits_{j=1}^{m-1}T_j(X))$\n",
    "    2. Fit max depth $d$ tree $T_m$ with features $X$, target $r_{m-1}$\n",
    "  \n",
    "Obtain final model: $F(X) = T_0 + T_1(X) + ... + T_k(X)$\n",
    "\n",
    "![](images/RegressDiagram.png)\n",
    "\n",
    "Not too complicated! But hold on, where does the *gradient* part of the name come in? And would we be able to adapt this model to a classification problem? As it turns out, the answer to both questions is the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generalizing GBM to Differentiable Loss Functions**:\n",
    "\n",
    "Yep, the **gradient** bit is closely related to the gradient descent algorithm that we know and love. It's also where the real power of the GBM framework becomes apparent. With the more general version of the algorithm that we'll see below, we can simply plug in a differentiable loss function and get an appropriate model.\n",
    "\n",
    "To understand the general approach, consider the prediction values $F(X)$ that approximate the target $y$ for an arbitrary model $F$. Without putting any constraints on the function $F$, we could choose a differentiable loss function $L(y_{true},y_{pred})$ and define the model that minimizes it:\n",
    "\n",
    "$$F_* = argmin_{F}L(y,F(X))$$\n",
    "\n",
    "But since our loss function $L$ is differentiable, we can solve this problem by using iterative gradient descent on an element-wise basis:\n",
    "\n",
    "$$F_{m+1}(x_i) = F_m(x_i) - \\alpha\\nabla{L(y_i,F_m(x_i))}$$ \n",
    "\n",
    "There's a clear problem here. All we're doing above is adjusting each predicted $y_i$ value until it gets increasingly close to the true $y_i$, meaning that we're memorizing the training data and overfitting. To not overfit, we have to introduce some constraint that forces generalization. In this case, instead of using the pure gradient, we'll model the gradient at each step with a regression tree $T_{m+1}$, fit on $X$ as features and the individual gradients $-\\frac{\\partial}{\\partial{F_m(x_i)}}L(y_{i},F_m(x_i))$ as targets. Doing this, we can modify the gradient descent update rule so that it becomes:\n",
    "\n",
    "$$F_{m+1}(x_i) = F_m(x_i) + T_{m+1}(x_i)$$\n",
    "\n",
    "This looks exactly like the algorithm we used for regression boosting. It seems a bit fancier than what we had earlier, but really the only difference is that we've substituted gradient terms (**pseudo-residuals**) for the simple residuals we had originally. In fact, for SSE regression as above the gradient terms are equal to the residuals, so it's simply a special case of this general version of boosting. And with this single change we've expanded from being able to model least squares regression problems to being able to model for any differentiable loss function.    \n",
    "\n",
    "Here is our generalized algorithm:\n",
    "\n",
    "1. Choose loss function $L(y_{true},y_{pred})$\n",
    "2. Initialize with best constant prediction i.e. $F_0 = argmin_{\\gamma}\\sum\\limits_{i=1}^{N}L(y_i,\\gamma)$\n",
    "2. For $m = 1,...,k$:\n",
    "    1. For $i = 1,...,n$: Set $r_{m-1,i} = -\\frac{\\partial{L(y_{i},F_{m-1}(x_i))}}{\\partial{F_{m-1}(x_i)}}$\n",
    "    2. Fit max depth $d$ tree $T_m$ with features $X$, target $r_{m-1}$\n",
    "    3. Update $F_m(x) = F_{m-1}(x) + T_m(x)$\n",
    "  \n",
    "Obtain final model: $F(X) = T_0 + T_1(X) + ... + T_k(X)$\n",
    "\n",
    "Note that we adapt to $k > 2$ class classification problems by training k of these models using [softmax loss](https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of GBMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted tree models are often the gold standard for tabular data in modern machine learning. They are well-equipped to model complex ground-truth functions without overfitting, adapt to your loss function of choice, and scale well to large datasets. They tend to do quite well with messy data: tree methods handle missing values naturally and can filter out the least useful input variables based on feature selection for splitting. That said, they are usually more sensitive to hyperparameter choice than random forests and can be a bit tricky and time-consuming to tune. They also lack the explicit interpretability of simpler models like linear/logistic regression.\n",
    "\n",
    "In short, GBMs are a powerful tool to apply to a wide range of datasets (in terms of loss objective, size, variable types, and messiness). They are probably the closest thing to a catch-all and out-of-the-box modeling approach that currently exists. In a use case where you have at least a reasonable amount of data, want an optimally predictive model, can sacrifice some interpretability, and have the time/resources to tune multiple hyperparameters, you should definitely try a GBM model.   \n",
    "\n",
    "![](images/xgbcalm.png)\n",
    "\n",
    "**Summary**\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "* Adaptable to wide range of problem settings (regression, classification, unusual or custom loss functions)\n",
    "* Potential to be highly expressive (model compex, non-linear relationships)...\n",
    "* But still flexible enough to accomodate complexity constraints/regularization  \n",
    "\n",
    "*General tree benefits:*\n",
    "\n",
    "* Feature scale invariant / natural handling of heterogenous features\n",
    "* Automatic handling of missing/NA values \n",
    "* Automatic feature interaction\n",
    "* Robust to input outliers and unhelpful input features (automatic feature selection)\n",
    "* Scales well to large datasets\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "* Many hyperparameters to tune for full power -- can be time-consuming and cause overfitting if not well-validated\n",
    "* Lack clear/clean interpretability\n",
    "* Tree methods struggle with extrapolation (can perform poorly on feature values outside of training range) \n",
    "* Can struggle with handling categorical variables (especially if large # of categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands on GBMs and parameter tuning\n",
    "\n",
    "Now that we've covered some of the theory behind GBMS and their pros and cons, let's turn to a concrete example of training one. We'll use [xgboost](https://github.com/dmlc/xgboost) (extreme gradient boosting), currently the most popular python package when it comes to training production-level boosting models. See [here](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn) for documentation on the sklearn-style interface for XGBoost that we'll be using.\n",
    "\n",
    "Another popular option is Microsoft's [lightgbm](https://github.com/Microsoft/LightGBM) package, which is  faster and sometimes more accurate than xgboost, but typically harder to tune and closer to bleeding edge as software. Sci-kit learn does have an easier to use [gbm implementation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), but it's less efficient and less powerful.\n",
    "\n",
    "Xgboost makes some adjustments to the algorithm described above and introduces a number of powerful efficiencies that allow the package to scale well. [This post](https://medium.com/@Synced/tree-boosting-with-xgboost-why-does-xgboost-win-every-machine-learning-competition-ca8034c0b283) is a good overview comparison of classic gradient boosting and xgboost, and the [xgboost paper](https://arxiv.org/pdf/1603.02754.pdf) digs into methodology and implementation details.\n",
    "\n",
    "For this example we'll use the California housing dataset that we've seen before (regression on housing prices). We'll first look at a couple of benchmarks, then step through the process of tuning an xgboost model and see what it can do. The process would be essentially the same for a classification problem (part of what makes boosting so great), which we'll see in a quick example at the end on the Wisconsin breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_dataset = fetch_california_housing()\n",
    "X = pd.DataFrame(housing_dataset.data)\n",
    "X.columns = housing_dataset.feature_names\n",
    "y = housing_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Split data into 3: 60% train, 20% validation, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2018)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=2019)\n",
    "\n",
    "#Evaluate models with Root Mean Squared Error\n",
    "def rmse(actuals, preds):\n",
    "    return np.sqrt(((actuals - preds) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "rmse(lr.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 100, max_features = 3, n_jobs=-1)\n",
    "rf.fit(X_train,y_train)\n",
    "rmse(rf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Xgboost Tuning**\n",
    "\n",
    "Now that we've set a solid baseline, let's turn to xgboost. Most of the parameters we'll use here should be familiar from the first section of this notebook or similar to what we've seen for random forests, with the exception of *learning rate*.\n",
    "\n",
    "**n_estimators**: number of base learner trees  \n",
    "**max_depth**: max depth per base tree (typical values are 3-12)   \n",
    "**learning_rate**: shrinkage factor applied to each base tree update  \n",
    "**subsample**: row subsampling rate (similar to RF)   \n",
    "**min_child_weight**: roughly the minimum allowable child samples for a tree split to occur  \n",
    "**colsample_bytree**: feature subsampling rate (similar to RF) \n",
    "\n",
    "The use of a learning rate/shrinkage factor is a form of regularization that can greatly reduce overfitting. It typically trades off with the n_estimators and depth parameters (raising these add complexity) -- lower learning rate  usually wants higher n_estimators, higher max depth usually wants lower learning rate etc. The two subsampling parameters and min_child_weight are also forms of regularization. These types of tradeoffs are part of why it typically works better to follow a manual tuning procedure than to try a massive grid search across different parameter combinations. That simply doesn't scale well to large datasets. \n",
    "\n",
    "Instead of trying to pick n_estimators by hand or do an intensive grid search, we'll use the **early stopping** technique as a massive time saver. This means that we track our validation error at each update step in the training process, and halt training once validation error hasn't improved in some number of **early_stopping_rounds**. This way, we can set n_estimators to a very large number and simply train until we land at a good stopping point for validation error. \n",
    "\n",
    "(One technical problem with early stopping is that n_estimators selection can overfit to the validation data. One reasonable solution is to combine early stopping with k-fold cross validation, and use the average n_estimators across k training rounds for training a new model).   \n",
    "\n",
    "Here is our first xgboost example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = xgb.XGBRegressor( \n",
    "                       n_estimators=30000, #arbitrary large number\n",
    "                       max_depth=3,\n",
    "                       objective=\"reg:squarederror\",  # Other options: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "                       learning_rate=.1, \n",
    "                       subsample=1,\n",
    "                       min_child_weight=1,\n",
    "                       colsample_bytree=.8\n",
    "                      )\n",
    "\n",
    "eval_set=[(X_train,y_train),(X_val,y_val)] #tracking train/validation error as we go\n",
    "fit_model = gbm.fit( \n",
    "                    X_train, y_train, \n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric='rmse',\n",
    "                    early_stopping_rounds=20,\n",
    "                    verbose=True #gives output log as below\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting xgb benchmark: we've already improved on RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using n_estimators with lowest validation error\n",
    "rmse(gbm.predict(X_test, ntree_limit=gbm.best_ntree_limit),y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune, we should use validation results and ignore test until final verification. So here's the validation error benchmark we want to beat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(gbm.predict(X_val, ntree_limit=gbm.best_ntree_limit),y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Xgboost Exercise**\n",
    "\n",
    "We'll approach tuning using this general framework, starting from the original parameters:\n",
    "\n",
    "1. Tune max_depth\n",
    "2. Tune subsample\n",
    "3. Tune min_child_weight\n",
    "4. Tune colsample_bytree\n",
    "5. Set learning_rate to .05 and use early stopping to get n_estimators\n",
    "\n",
    "There are even more [xgboost parameters](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst) such as regularization terms that are worth tuning, but this framework should be a starting point for solid parameters.\n",
    "\n",
    "For this process we'll repeat the cell block below with each adjustment and track our progress with comments, and tune in this way: say for max_depth, starting from 3 we try 5, 7 etc. until we stop improving then narrow down the search space.\n",
    "\n",
    "Start by _replacing the final parameters below_ with the basic parameters listed in the first run above, and see if you can recover parameters close to those referenced here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step by step RMSEs, with .1 learning rate:\n",
    "#best max_depth: 7, .452\n",
    "#best subsample: .8, .448\n",
    "#best min_child_weight: 12, .446\n",
    "#best colsample_bytree: .7, .444\n",
    "\n",
    "gbm = xgb.XGBRegressor(\n",
    "    n_estimators=30000,  #arbitrary large number\n",
    "    max_depth=7,\n",
    "    objective=\"reg:squarederror\",\n",
    "    learning_rate=.05,\n",
    "    subsample=.8,\n",
    "    min_child_weight=12,\n",
    "    colsample_bytree=.7,\n",
    "    n_jobs=-1,\n",
    "    random_state=0)\n",
    "\n",
    "eval_set = [(X_train, y_train),\n",
    "            (X_val, y_val)]  #tracking train/validation error as we go\n",
    "fit_model = gbm.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=eval_set,\n",
    "    eval_metric='rmse',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False)\n",
    "rmse(gbm.predict(X_val, ntree_limit=gbm.best_ntree_limit), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our test MSE as a result of this process, a ~10% reduction in RMSE from the random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(gbm.predict(X_test, ntree_limit=gbm.best_ntree_limit),y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Challenge**\n",
    "\n",
    "If you'd like to challenge yourself to get more proficient with xgboost, try tuning this model even more and beat our current score. Do some research into other xgboost parameters that you could try using. I'd recommend l1, l2, and gamma regularization terms. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "When trying to interpret a boosting model, we're faced with a similar challenge to that of interpreting a RF model. Many people would refer to it as a \"black box\" model. Luckily, there are some readily available ways to better understand feature importances and impacts on the model.  \n",
    "\n",
    "The easiest method is very similar to RF feature importances. We can assign importance scores to features in a number of different ways, and xgboost lets us easily plot/extract these scores. The most common scoring methods are **frequency** (default, just calculates the # of total tree splits the feature is used in), and **gain** (probably better, calculates cumulative information gain from tree splits done on that feature).\n",
    "\n",
    "As we can see in the charts below, the two methods tend to agree on which features are most important. However, gain views Median Income as much more important than any of the other features, which seems like a reasonable conclusion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(gbm)\n",
    "xgb.plot_importance(gbm, importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.get_booster().get_score(importance_type='weight') #extract raw frequency scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.get_booster().get_score(importance_type='gain') #extract raw gain scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key problem with importance scores is that they only tell us which features are most relevant, not what the directional impact of the feature is on the model/target. Techniques have been developed to visually study these impacts that include [Partial Dependence](http://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html) and [Individual Conditional Expectation](https://arxiv.org/pdf/1309.6392.pdf) plots, but these are beyond the scope of this lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Example**\n",
    "\n",
    "Breast cancer dataset (binary classification). Boosting does not seem to work that well on this problem (unsuprising given a small sample size of <600). But this shows how we can apply xgboost to a classification problem by simply changing the objective and eval_metric parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_dataset = load_breast_cancer()\n",
    "X = pd.DataFrame(bc_dataset.data)\n",
    "X.columns = bc_dataset.feature_names\n",
    "y = bc_dataset.target\n",
    "\n",
    "#Split data into 3: 60% train, 20% validation, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = xgb.XGBClassifier( \n",
    "                        n_estimators=30000,\n",
    "                        max_depth=4,\n",
    "                        objective='binary:logistic', #new objective\n",
    "                        learning_rate=.05, \n",
    "                        subsample=.8,\n",
    "                        min_child_weight=3,\n",
    "                        colsample_bytree=.8\n",
    "                       )\n",
    "\n",
    "eval_set=[(X_train,y_train),(X_val,y_val)]\n",
    "fit_model = gbm.fit( \n",
    "                    X_train, y_train, \n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric='error', #new evaluation metric: classification error (could also use AUC, e.g.)\n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose=False\n",
    "                   )\n",
    "\n",
    "accuracy_score(y_test, gbm.predict(X_test, ntree_limit=gbm.best_ntree_limit)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('classification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "5f0e1bddb7ed37f67f576e4a6a21b2408ebab9a006050dc6d038d31036c144c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
